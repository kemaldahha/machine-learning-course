{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 [Credit risk scoring project](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/06-trees/01-credit-risk.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week we'll focus on a credit risk scoring model. For example if a customer wants to borrow money from a bank, the bank can decide whether to lend the money or not based on some information. The model predicts a risk that a customer would default on their loan.\n",
    "\n",
    "The model is trained on historical data on people who got a loan and defaulted or not. The model is a binary classification model, similar to last week's customer churn model.\n",
    "\n",
    "$$\n",
    "y_i \\in \\{0, 1\\}\n",
    "$$\n",
    "\n",
    "0 means the customer did not default, 1 means the customer did default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.2 [Data cleaning and preparation](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/06-trees/02-data-prep.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/gastonstat/CreditScoring/raw/master/CreditScoring.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"CreditScoring.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll lower case the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll substitute the numerical values to string, such that we know what the different categories are instead of seeing numbers. To this end we use `df.map` which takes in a mapping dictionary as argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_values = {\n",
    "    1: 'rent',\n",
    "    2: 'owner',\n",
    "    3: 'private',\n",
    "    4: 'ignore',\n",
    "    5: 'parents',\n",
    "    6: 'other',\n",
    "    0: 'unk'\n",
    "}\n",
    " \n",
    "marital_values = {\n",
    "    1: 'single', \n",
    "    2: 'married', \n",
    "    3: 'widow', \n",
    "    4: 'separated',\n",
    "    5: 'divorced',\n",
    "    0: 'unk'\n",
    "}\n",
    " \n",
    "records_values = {\n",
    "    1: 'no',\n",
    "    2: 'yes',\n",
    "    0: 'unk'\n",
    "}\n",
    " \n",
    "job_values = {\n",
    "    1: 'fixed', \n",
    "    2: 'partime', \n",
    "    3: 'freelance', \n",
    "    4: 'others',\n",
    "    0: 'unk'\n",
    "}\n",
    "\n",
    "df.records = df.records.map(records_values)\n",
    "df.marital = df.marital.map(marital_values)\n",
    "df.home = df.home.map(home_values)\n",
    "df.job = df.job.map(job_values)\n",
    " \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`income`, `assets`, and `debt` have very large values. These are actually missing values. We will replace them by `np.nan`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for c in [\"income\", \"assets\", \"debt\"]:\n",
    "    df.loc[df[c]==99999999, c] = np.nan\n",
    "\n",
    "df[[\"income\", \"assets\", \"debt\"]].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have one sample with unknown status. We will just drop it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"status\"]!=0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we'll change the encoding. `2` means the customer defaulted so it should become `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.status = (df.status==2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=11)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_train = df_full_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.status.values\n",
    "y_val = df_val.status.values\n",
    "y_test = df_test.status.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the target variable from our dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train[\"status\"]\n",
    "del df_val[\"status\"]\n",
    "del df_test[\"status\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.3 [Decision trees](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/06-trees/03-decision-trees.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree consists of conditions based on which an outcome is predicted. See example below. It's a bunch of if-then-else rules.\n",
    "\n",
    "<img src=decisiontree.png width=600>\n",
    "\n",
    "This is an example. If we were to write this in code, it would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_risk(client):\n",
    "    if client[\"records\"] == \"yes\":\n",
    "        if client[\"job\"] == \"parttime\":\n",
    "            return \"default\"\n",
    "        else:\n",
    "            return \"ok\"\n",
    "    else:\n",
    "        if client[\"assets\"] > 6000:\n",
    "            return \"ok\"\n",
    "        else:\n",
    "            return \"default\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this decision tree on the first record in `df_train`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = df_train.iloc[0].to_dict()\n",
    "\n",
    "assess_risk(xi)\n",
    "\n",
    "# y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we encoded a set of rules in the decision tree above, we can also learn the if-then-else rules using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dicts = df_train.fillna(0).to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer(sparse=False)\n",
    "X_train = dv.fit_transform(train_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check our roc auc score on the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dicts = df_val.fillna(0).to_dict(orient=\"records\")\n",
    "X_val = dv.transform(val_dicts)\n",
    "\n",
    "y_pred = dt.predict_proba(X_val)[:, 1]\n",
    "\n",
    "roc_auc_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not great. Let's check it for our training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt.predict_proba(X_train)[:, 1]\n",
    "\n",
    "roc_auc_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC score on the training dataset is perfect, whereas on the validation dataset it's poor. Whatever the model learned on the training dataset, does not translate well to unseen data in the validation dataset. This is called **overfitting**.\n",
    "\n",
    "What happens is that the model creates a rule for each customer in the training dataset. But this pattern is not true in general. Our model is said to have low bias and high variance. Right now we have not restricted the depth of the decision tree. What this means is that it can create as many conditionals as it wants to fit the data. We end up with overly specific rules which are reprenting the individual samples in the training set, rather then representing any general mechanisms. What we can do is to constrain the decision tree depth. This will give us rules that are less specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt.predict_proba(X_train)[:, 1]\n",
    "roc_auc_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt.predict_proba(X_val)[:, 1]\n",
    "roc_auc_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with a `max_depth` of 3, we have better performance on the validation dataset and it is in line with the training dataset as well, suggesting whatever we learned from the training datset, generalizes to unseen data.\n",
    "\n",
    "We can visualize the rules that our model came up with by visualizing the decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "print(export_text(dt, feature_names=dv.feature_names_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 [Decision tree learning algorithm](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/06-trees/04-decision-tree-learning.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does a decision tree come up with rules? Let's look at this example dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [8000, \"default\"],\n",
    "    [2000, \"default\"],\n",
    "    [0, \"default\"],\n",
    "    [5000, \"ok\"],\n",
    "    [5000, \"ok\"],\n",
    "    [4000, \"ok\"],\n",
    "    [9000, \"ok\"],\n",
    "    [3000, \"default\"],\n",
    "]\n",
    "\n",
    "df_example = pd.DataFrame(data, columns=[\"assets\", \"status\"])\n",
    "df_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a decision tree to split based on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_example.sort_values(\"assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ts = [0, 2000, 3000, 4000, 5000, 8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for T in Ts:\n",
    "    df_right = df_example[df_example.assets > T]\n",
    "    df_left = df_example[df_example.assets <= T]\n",
    "    display(df_left)\n",
    "    display(df_right)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on different thresholds, we have various splits. We can for example decide on a rule based on majority. Let's take `T=4000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 4000\n",
    "df_right = df_example[df_example.assets > T]\n",
    "df_left = df_example[df_example.assets <= T]\n",
    "display(df_left)\n",
    "display(df_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that `df_left` mostly has status `default`, whereas `df_right` mostly has status `ok`. Let's take a look at the misclassification rate as a metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_right.status != df_right.status.mode()[0]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_left.status != df_left.status.mode()[0]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a weighted average of these, yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(df_right) * (df_right.status != df_right.status.mode()[0]).mean() + len(df_left) * (df_left.status != df_left.status.mode()[0]).mean()) / len(df_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The misclassification rate is a measure of **impurity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>decision left</th>\n",
       "      <th>impurity left</th>\n",
       "      <th>decision right</th>\n",
       "      <th>impurity right</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>default</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.37625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>default</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.37625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>default</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.24750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      T decision left  impurity left decision right  impurity right  average\n",
       "0   500       default            0.0             ok            0.43  0.37625\n",
       "1  1000       default            0.0             ok            0.43  0.37625\n",
       "2  2000       default            0.0             ok            0.33  0.24750"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = [(\"T\", \"decision left\", \"impurity left\", \"decision right\", \"impurity right\", \"average\")]\n",
    "for T in Ts:\n",
    "    df_right = df_example[df_example.assets > T]\n",
    "    df_left = df_example[df_example.assets <= T]\n",
    "    table.append(\n",
    "        (T,\n",
    "        df_left.status.mode()[0],\n",
    "        (df_left.status != df_left.status.mode()[0]).mean().round(2),\n",
    "        df_right.status.mode()[0],\n",
    "        (df_right.status != df_right.status.mode()[0]).mean().round(2),\n",
    "        ((df_left.status != df_left.status.mode()[0]).mean().round(2)*len(df_left) + (df_right.status != df_right.status.mode()[0]).mean().round(2)*len(df_right))/(len(df_example)),\n",
    "    )) \n",
    "pd.DataFrame(table[1:], columns=table[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best split is for `T=3000`, since it has the lowest weighted average impurity.\n",
    "\n",
    "We will now add a second feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [8000, 3000, \"default\"],\n",
    "    [2000, 1000, \"default\"],\n",
    "    [0, 1000, \"default\"],\n",
    "    [5000, 1000, \"ok\"],\n",
    "    [5000, 1000, \"ok\"],\n",
    "    [4000, 1000, \"ok\"],\n",
    "    [9000, 500, \"ok\"],\n",
    "    [3000, 2000, \"default\"],\n",
    "]\n",
    "\n",
    "df_example = pd.DataFrame(data, columns=[\"assets\", \"debt\", \"status\"])\n",
    "df_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_example.sort_values(\"debt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>T</th>\n",
       "      <th>decision left</th>\n",
       "      <th>impurity left</th>\n",
       "      <th>decision right</th>\n",
       "      <th>impurity right</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>assets</td>\n",
       "      <td>0</td>\n",
       "      <td>default</td>\n",
       "      <td>0.00</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.37625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>assets</td>\n",
       "      <td>2000</td>\n",
       "      <td>default</td>\n",
       "      <td>0.00</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.24750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>assets</td>\n",
       "      <td>3000</td>\n",
       "      <td>default</td>\n",
       "      <td>0.00</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.12500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>assets</td>\n",
       "      <td>4000</td>\n",
       "      <td>default</td>\n",
       "      <td>0.25</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>assets</td>\n",
       "      <td>5000</td>\n",
       "      <td>default</td>\n",
       "      <td>0.50</td>\n",
       "      <td>default</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>assets</td>\n",
       "      <td>8000</td>\n",
       "      <td>default</td>\n",
       "      <td>0.43</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>debt</td>\n",
       "      <td>500</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.00</td>\n",
       "      <td>default</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.37625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>debt</td>\n",
       "      <td>1000</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.33</td>\n",
       "      <td>default</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.24750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>debt</td>\n",
       "      <td>2000</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.43</td>\n",
       "      <td>default</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature     T decision left  impurity left decision right  impurity right  \\\n",
       "0  assets     0       default           0.00             ok            0.43   \n",
       "1  assets  2000       default           0.00             ok            0.33   \n",
       "2  assets  3000       default           0.00             ok            0.20   \n",
       "3  assets  4000       default           0.25             ok            0.25   \n",
       "4  assets  5000       default           0.50        default            0.50   \n",
       "5  assets  8000       default           0.43             ok            0.00   \n",
       "6    debt   500            ok           0.00        default            0.43   \n",
       "7    debt  1000            ok           0.33        default            0.00   \n",
       "8    debt  2000            ok           0.43        default            0.00   \n",
       "\n",
       "   average  \n",
       "0  0.37625  \n",
       "1  0.24750  \n",
       "2  0.12500  \n",
       "3  0.25000  \n",
       "4  0.50000  \n",
       "5  0.37625  \n",
       "6  0.37625  \n",
       "7  0.24750  \n",
       "8  0.37625  "
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds = {\n",
    "    \"assets\": [0, 2000, 3000, 4000, 5000, 8000],\n",
    "    \"debt\": [500, 1000, 2000],\n",
    "}\n",
    "\n",
    "table = [(\"feature\", \"T\", \"decision left\", \"impurity left\", \"decision right\", \"impurity right\", \"average\")]\n",
    "\n",
    "for feature, Ts in thresholds.items():\n",
    "    # print(feature)\n",
    "    for T in Ts:\n",
    "        # print(\"#################\")\n",
    "        # print(feature, T)\n",
    "        \n",
    "        df_left = df_example[df_example[feature] <= T]\n",
    "        df_right = df_example[df_example[feature] > T]\n",
    "        \n",
    "        # display(df_left)\n",
    "        # print(df_left.status.value_counts(normalize=True))\n",
    "        \n",
    "        # display(df_right)\n",
    "        # print(df_right.status.value_counts(normalize=True))\n",
    "\n",
    "        table.append(\n",
    "            (\n",
    "                feature,\n",
    "                T,\n",
    "                df_left.status.mode()[0],\n",
    "                (df_left.status != df_left.status.mode()[0]).mean().round(2),\n",
    "                df_right.status.mode()[0],\n",
    "                (df_right.status != df_right.status.mode()[0]).mean().round(2),\n",
    "                ((df_left.status != df_left.status.mode()[0]).mean().round(2)*len(df_left) + (df_right.status != df_right.status.mode()[0]).mean().round(2)*len(df_right))/(len(df_example)),\n",
    "            )\n",
    "        ) \n",
    "pd.DataFrame(table[1:], columns=table[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see here is that the feature `assets` with a weighted average of impurity of 12.5% is the best choice.\n",
    "\n",
    "If we'd have to summarize the decision tree algorithm it's as follows:\n",
    "\n",
    "```\n",
    "for f in features:\n",
    "    find all thresholds for f\n",
    "        for t in thresholds\n",
    "            split dataset using f>t\n",
    "                compute impurity of this split\n",
    "select condition with lowest impurity\n",
    "```\n",
    "\n",
    "We can split recursively. But if we split too much, we risk overfitting to the dataset. We need to know when to stop splitting. There are some stopping criteria:\n",
    "1. group is already pure\n",
    "1. decision tree reaches `max_depth`\n",
    "1. group too small to split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.5 [Decision trees parameter tuning](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/06-trees/05-decision-tree-tuning.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.6 [Ensemble learning and random forest](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/06-trees/06-random-forest.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.7 [Gradient boosting and XGBoost](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/06-trees/07-boosting.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.8 [XGBoost parameter tuning](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/06-trees/08-xgb-tuning.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.9 [Selecting the best model](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/06-trees/09-final-model.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.10 [Summary](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/06-trees/10-summary.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.11 [Explore more](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/06-trees/11-explore-more.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.12 [Homework](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/06-trees/homework.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-zoomcamp-py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
